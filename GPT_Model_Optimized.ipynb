{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## GPT Model trained on tiny-Shakespeare data and Optimized performance and efficiency\nThis is a separate notebook for experimenting with the model.\nKey Components of the model are:\n1. word embedding: **Sub-word level tokens**\n2. Positional encoding: **Rotational PE**\n3. multi-headed self-attention (No cross attention): **GeLU activation function**\n4. Transformer Decoder block (No Encoder since it is generating text on its own) - **RMS Normalization**\n\nOther Optimization Implemented during Training:\n- Weight Decay\n- Learning Rate Scheduling\n- Gradient Clipping\n- Mixed precision training using Pytoch's Automatic Mixed Precision package","metadata":{"id":"dtPsWQFcbIZY"}},{"cell_type":"code","source":"# Download the tiny-Shakesspeare text data from Github repo\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fl4cSu9Ddeqp","outputId":"356c3a5a-7d3f-466e-ce1b-cbbc263ad2f2","execution":{"iopub.status.busy":"2023-12-21T07:14:31.952477Z","iopub.execute_input":"2023-12-21T07:14:31.953311Z","iopub.status.idle":"2023-12-21T07:14:33.037825Z","shell.execute_reply.started":"2023-12-21T07:14:31.953274Z","shell.execute_reply":"2023-12-21T07:14:33.036917Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2023-12-21 07:14:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt.3'\n\ninput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n\n2023-12-21 07:14:32 (17.8 MB/s) - 'input.txt.3' saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport time\nfrom torch.cuda.amp import GradScaler, autocast # For mixed precision Training\n#import torch_xla\n#import torch_xla.core.xla_model as xm","metadata":{"id":"tb2m_FMD-vvE","execution":{"iopub.status.busy":"2023-12-21T07:14:33.116459Z","iopub.execute_input":"2023-12-21T07:14:33.116763Z","iopub.status.idle":"2023-12-21T07:14:34.551346Z","shell.execute_reply.started":"2023-12-21T07:14:33.116736Z","shell.execute_reply":"2023-12-21T07:14:34.550548Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Import data and Explore","metadata":{"id":"nzSArLnwq8Kd"}},{"cell_type":"code","source":"# Read the tiny-Shakespeare txt file\nwith open('input.txt',mode='r',encoding='utf-8',closefd=True) as f:\n  text = f.read()","metadata":{"id":"u2_fqE-GlyVf","execution":{"iopub.status.busy":"2023-12-21T07:14:34.552802Z","iopub.execute_input":"2023-12-21T07:14:34.553168Z","iopub.status.idle":"2023-12-21T07:14:34.558805Z","shell.execute_reply.started":"2023-12-21T07:14:34.553141Z","shell.execute_reply":"2023-12-21T07:14:34.557986Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# View file and its stats\nprint(type(text),len(text),text[1:100])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3v2DxpFwnEU-","outputId":"5607899f-7b50-40d9-86e8-b18334fd4806","execution":{"iopub.status.busy":"2023-12-21T07:14:34.908340Z","iopub.execute_input":"2023-12-21T07:14:34.908696Z","iopub.status.idle":"2023-12-21T07:14:34.914003Z","shell.execute_reply.started":"2023-12-21T07:14:34.908667Z","shell.execute_reply":"2023-12-21T07:14:34.913002Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'str'> 1115394 irst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n","output_type":"stream"}]},{"cell_type":"code","source":"# List all unique characters used in the text\nchar1 = set(text)\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars),vocab_size,char1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lgM-6QInIv2","outputId":"b66c192d-f297-4afc-c803-94169ff96620","execution":{"iopub.status.busy":"2023-12-21T07:14:35.406069Z","iopub.execute_input":"2023-12-21T07:14:35.406378Z","iopub.status.idle":"2023-12-21T07:14:35.443705Z","shell.execute_reply.started":"2023-12-21T07:14:35.406354Z","shell.execute_reply":"2023-12-21T07:14:35.442708Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 65 {\"'\", '$', 'H', 'P', 'l', 's', 'g', 'C', ':', 'G', 'b', ' ', 'D', 'K', 'v', 'J', 'S', ';', 'u', 'r', 'y', 'T', 'E', 'L', 'n', 'F', '!', '3', 'a', '&', '\\n', 'f', 'O', 'B', 'w', 'q', 'm', 'p', 'U', '?', 'j', 'x', 'Q', 'Y', 'A', 'I', 'N', ',', '.', 'o', 'd', 'W', 'i', 'R', 'z', 'M', 'e', '-', 'Z', 't', 'k', 'X', 'V', 'c', 'h'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Tokenize the text data","metadata":{"id":"1fCEfuKcrE7W"}},{"cell_type":"code","source":"#print(''.join(chars),vocab_size,char1) # Create a mapping of characters and integers\nstoi = {ch:i for i,ch in enumerate(chars)}\nitos = {i:ch for i,ch in enumerate(chars)}\n#print(stoi['A'],'\\n', itos)\n\n# Encoder takes string as input and provides integers as output.\nencode = lambda s: [stoi[c] for c in s]\n\n# Decoder takes list of integers as input an provides string as output\ndecode = lambda l: ''.join([itos[i] for i in l])\n\n#print(encode('Hi. I am Nimish!!'))\n#print(decode(encode('Hi. I am Nimish!!')))\n","metadata":{"id":"GuwvQ1J-Sp-m","execution":{"iopub.status.busy":"2023-12-21T07:14:36.561107Z","iopub.execute_input":"2023-12-21T07:14:36.561468Z","iopub.status.idle":"2023-12-21T07:14:36.567582Z","shell.execute_reply.started":"2023-12-21T07:14:36.561436Z","shell.execute_reply":"2023-12-21T07:14:36.566661Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Lets encode entire text dataset and store it into a torch.tensor\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape,'\\n',data.dtype,'\\n',data[:100])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oE5qlTyfynSq","outputId":"e85b6f97-9775-4a94-c0f3-8beadb149276","execution":{"iopub.status.busy":"2023-12-21T07:14:37.062953Z","iopub.execute_input":"2023-12-21T07:14:37.063248Z","iopub.status.idle":"2023-12-21T07:14:37.315275Z","shell.execute_reply.started":"2023-12-21T07:14:37.063221Z","shell.execute_reply":"2023-12-21T07:14:37.314337Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([1115394]) \n torch.int64 \n tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Data prep for training\nSplit the data in to Train and Val (10%)\nDivide the data in to chunks/blocks and batches","metadata":{"id":"etNYLOt556uJ"}},{"cell_type":"code","source":"# Divide the data in to train (90%) and validation set (10%)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"id":"EvbB-cKJ0ewF","execution":{"iopub.status.busy":"2023-12-21T07:14:38.051072Z","iopub.execute_input":"2023-12-21T07:14:38.051458Z","iopub.status.idle":"2023-12-21T07:14:38.056592Z","shell.execute_reply.started":"2023-12-21T07:14:38.051423Z","shell.execute_reply":"2023-12-21T07:14:38.055566Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n\n    # Create batch of 4 randomly generated integers within length of text data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n\n    # Create batch of 4 blocks (each of block size 8) from randomly selected integers for parallel processing\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n\n    #move x, y parameters to GPU if available or on CPU\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"id":"h_-m6llh8As1","execution":{"iopub.status.busy":"2023-12-21T07:14:38.531912Z","iopub.execute_input":"2023-12-21T07:14:38.532269Z","iopub.status.idle":"2023-12-21T07:14:38.538698Z","shell.execute_reply.started":"2023-12-21T07:14:38.532238Z","shell.execute_reply":"2023-12-21T07:14:38.537802Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    \n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        \n        # Evaluation is run for eval_iters iteration and computes losses for each iteration\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            \n            \n            # Loss for each batch is averaged over GPU tensors and stored as scalar in losses tensor at k index\n            #losses[k] = loss.item()\n            losses[k] = loss.mean() #Modified for GPU parallelism\n        \n        \n        \n        # The mean of losses across the evaluation iterations is stored for current data split\n        #out[split] = losses.mean()\n        out[split] = losses.mean().item() # Modified for Parallel GPU's to convert tensor to scalar\n    \n    model.train() # Model is switched back to train mode for further training\n    return out # The function returns the out dictionary, which contains the average losses for the training and validation sets.","metadata":{"id":"YhBy-hLRDgPq","execution":{"iopub.status.busy":"2023-12-21T07:14:39.442052Z","iopub.execute_input":"2023-12-21T07:14:39.442382Z","iopub.status.idle":"2023-12-21T07:14:39.448509Z","shell.execute_reply.started":"2023-12-21T07:14:39.442353Z","shell.execute_reply":"2023-12-21T07:14:39.447579Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False) # Key represents the information each element of sequence holds\n        self.query = nn.Linear(n_embd, head_size, bias=False) # Defines the query or search the model is asking for\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n\n        # masking out the attention scores in the upper triangular portion of the matrix, the model is forced to only focus on the elements that appear before the current position in the sequence.\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        # the model gives more weight to elements that have higher attention scores, meaning they are more relevant to the current query.\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out","metadata":{"id":"gFy5dUGPxrTM","execution":{"iopub.status.busy":"2023-12-21T07:14:39.939009Z","iopub.execute_input":"2023-12-21T07:14:39.939705Z","iopub.status.idle":"2023-12-21T07:14:39.950903Z","shell.execute_reply.started":"2023-12-21T07:14:39.939670Z","shell.execute_reply":"2023-12-21T07:14:39.950014Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define RMS Normalization Block\nclass RMSNorm(nn.Module):\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n        return self.scale * x / (rms + self.eps)","metadata":{"id":"SDahl8L21Ew4","execution":{"iopub.status.busy":"2023-12-21T07:14:40.459971Z","iopub.execute_input":"2023-12-21T07:14:40.460323Z","iopub.status.idle":"2023-12-21T07:14:40.466267Z","shell.execute_reply.started":"2023-12-21T07:14:40.460293Z","shell.execute_reply":"2023-12-21T07:14:40.465353Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(), ## GELU activation function\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size) #Initialization of self-multihead attention module defined earlier\n        self.ffwd = FeedFoward(n_embd) # Initialization of feedforward module defined earlier\n        #self.ln1 = nn.LayerNorm(n_embd) #Initialize first normalization layer\n        #self.ln2 = nn.LayerNorm(n_embd)  #Initialize second normalization layer\n        self.ln1 = RMSNorm(n_embd) #Initialize first normalization layer with RMS Norm\n        self.ln2 = RMSNorm(n_embd)  #Initialize second normalization layer with RMS Norm\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x)) #Input block (x) undergoes normalization followed by self attention\n        x = x + self.ffwd(self.ln2(x)) # Output of previous block goes through normalization followed by feedforward layer followed\n        return x","metadata":{"id":"WWs60ospyJDN","execution":{"iopub.status.busy":"2023-12-21T07:14:41.059243Z","iopub.execute_input":"2023-12-21T07:14:41.059522Z","iopub.status.idle":"2023-12-21T07:14:41.071709Z","shell.execute_reply.started":"2023-12-21T07:14:41.059499Z","shell.execute_reply":"2023-12-21T07:14:41.070952Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # Word Embedding\n        self.position_embedding_table = nn.Embedding(block_size, n_embd) # Positional Embedding\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Creates stack of n_layer transformer blocks using sequential container\n        self.ln_f = nn.LayerNorm(n_embd) # final layer normalization\n        self.lm_head = nn.Linear(n_embd, vocab_size) # Map normalized embedding outputs to logits which are used for loss calculation\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C) Token Embedding\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) Positional Embedding\n        x = tok_emb + pos_emb # (B,T,C) Embeddings added to create input sequence\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx","metadata":{"id":"E7uZJ9G5UJ6T","execution":{"iopub.status.busy":"2023-12-21T07:14:41.669431Z","iopub.execute_input":"2023-12-21T07:14:41.669746Z","iopub.status.idle":"2023-12-21T07:14:41.680996Z","shell.execute_reply.started":"2023-12-21T07:14:41.669720Z","shell.execute_reply":"2023-12-21T07:14:41.680285Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 512 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 200\nlearning_rate = 3e-4\neval_iters = 200\nn_embd = 384\nn_head = 8\nn_layer = 6\ndropout = 0.2\n\nmax_norm = 1.0 # maximum norm of the gradients. If the norm of the gradient exceeds this value, it will be scaled down to fit the norm limit.\nlr_decay = 0.2 # Learning rate decays to lr_decay*learning rate every 1000 steps\n\n#Optimizer parameters\nwt_decay = 0.1 # regularization technique that penalizes large weights during training.\nbeta1 = 0.9 #  decay rate of the first moment estimates, the exponentially weighted average of past gradients\nbeta2 = 0.95 # decay rate of the second moment estimates, which is the exponentially weighted average of past squared gradients\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Use TPU if available, otherwise use CPU\n# device = xm.xla_device() if xm.xla_device() is not None else 'cpu'\n","metadata":{"id":"M4tGpbi1BEtG","execution":{"iopub.status.busy":"2023-12-21T07:14:43.321735Z","iopub.execute_input":"2023-12-21T07:14:43.322366Z","iopub.status.idle":"2023-12-21T07:14:43.402523Z","shell.execute_reply.started":"2023-12-21T07:14:43.322330Z","shell.execute_reply":"2023-12-21T07:14:43.401588Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = BigramLanguageModel()\nmodel = nn.DataParallel(model)\nm = model.to(device)\n\nscaler = GradScaler() #  helps in adjusting the gradients to account for the reduced precision:\n\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=wt_decay, betas=(beta1, beta2))\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=lr_decay)\n\n# Start timing\nstat_time = time.time()\n\nfor iter in range(max_iters):\n\n    optimizer.zero_grad(set_to_none=True)  # Zero gradients at the start\n\n    xb, yb = get_batch('train')  # Get a batch of data\n    \n    # Computations within this block will be done in float16 wherever possible using autocast().\n    with autocast():\n        logits, loss = model(xb, yb)\n        loss = loss.mean()  # Ensure loss is a scalar (important for DataParallel), averaging over multiple GPU's\n\n    # Backward pass with scaled loss\n    scaler.scale(loss).backward()\n\n    # Unscale gradients and perform gradient clipping\n    scaler.unscale_(optimizer)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n\n    # Step with scaler (performs optimizer step and updates scaler)\n    scaler.step(optimizer)\n    scaler.update()\n\n    # Update learning rate\n    scheduler.step()\n\n    # Periodically evaluate loss\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Stop timing\nend_time = time.time()\n\n# Calculate and print training duration\ntraining_time = end_time - start_time\nprint(f\"Training completed in: {training_time:.2f} seconds\")\n\n# Generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.module.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-12-21T07:14:47.714909Z","iopub.execute_input":"2023-12-21T07:14:47.715271Z","iopub.status.idle":"2023-12-21T08:37:37.874999Z","shell.execute_reply.started":"2023-12-21T07:14:47.715241Z","shell.execute_reply":"2023-12-21T08:37:37.873660Z"},"_kg_hide-input":true,"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"10.882625 M parameters\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"step 0: train loss 3.7109, val loss 3.7242\nstep 200: train loss 2.4393, val loss 2.4739\nstep 400: train loss 2.2467, val loss 2.2952\nstep 600: train loss 2.0395, val loss 2.1214\nstep 800: train loss 1.8526, val loss 1.9870\nstep 1000: train loss 1.7098, val loss 1.8746\nstep 1200: train loss 1.6680, val loss 1.8449\nstep 1400: train loss 1.6441, val loss 1.8262\nstep 1600: train loss 1.6173, val loss 1.8003\nstep 1800: train loss 1.5923, val loss 1.7838\nstep 2000: train loss 1.5700, val loss 1.7647\nstep 2200: train loss 1.5639, val loss 1.7582\nstep 2400: train loss 1.5576, val loss 1.7530\nstep 2600: train loss 1.5543, val loss 1.7508\nstep 2800: train loss 1.5491, val loss 1.7497\nstep 3000: train loss 1.5457, val loss 1.7418\nstep 3200: train loss 1.5423, val loss 1.7399\nstep 3400: train loss 1.5423, val loss 1.7393\nstep 3600: train loss 1.5411, val loss 1.7388\nstep 3800: train loss 1.5394, val loss 1.7366\nstep 4000: train loss 1.5379, val loss 1.7397\nstep 4200: train loss 1.5396, val loss 1.7379\nstep 4400: train loss 1.5391, val loss 1.7391\nstep 4600: train loss 1.5390, val loss 1.7391\nstep 4800: train loss 1.5388, val loss 1.7383\nstep 4999: train loss 1.5373, val loss 1.7388\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate and print training duration\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m training_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m \u001b[43mstart_time\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Generate from the model\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"],"ename":"NameError","evalue":"name 'start_time' is not defined","output_type":"error"}]},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.module.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-12-21T09:04:53.765162Z","iopub.execute_input":"2023-12-21T09:04:53.766040Z","iopub.status.idle":"2023-12-21T09:05:41.450905Z","shell.execute_reply.started":"2023-12-21T09:04:53.766002Z","shell.execute_reply":"2023-12-21T09:05:41.449962Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\nCAMINIUS:\nBut the spersed:\nAnd heaven he call to thy lover-dangue\nEvooun steembren stoodd I my do awnin myou\nIn yet in the love frantured. Then let may prows: for\nTo father, noble and fathorsey consice what too I,\nwere though see the woulds of younsel moner.\nSaituout, our?\n\nHESTINBIO:\nSir.\nDYew, my grave Mores aster.\n\nDUKE VINCENV:\nI would rece, main:\nGody me kepboke;\nFor 't a upon alst; you suck yof you sin,\nWarick.\n\nNurse:\nWhat have is her, if her, my be is i' so?\n\nKING RICHARD II:\nA trie Lorcase couble, Plift I kneep your comb;\nThand hims that thous natle seeech ans an done\nOf this the surnnes itservisent.\nNo oupase you my Closingmans me\nHere therer comere; I not at your times,\nYoou prown the crembum for heldsency'd, tere\nTo yet tland my atals, neears, fir that sand Lucender,\nFors: what, he havirt faults, wish, all pusiltss\n'Take cretmain witde fir cripevibles.\nThat ex how he it; and not 'ow, this dong? wet that, not cusile, my by and:\nBensing you clasburs day amain?\nFathw heread him breity most of with the hert rine!ar\n\nGo not Lid which be with a fair that thy and was hinds.\nWhest, as the its offorbues of pray sin; at no a like.\nThat drospilres, unpry them anser ight. Thereat in be!\n\nLADWIDY turcking a thee, herty, a the, and the shalck; andatis,\nAndxiregnt on be toth criotiols selve\nWhat to may bein trulead,\nAnd remour brothing, and I doy zocer, Wandock, and ladss,\nThat, do sumbire our beheate foe, or thosat here broth?\n\nWARcat:\nHe bear is mortance, let lesend rout fiells,\nMy we vouly of maker fleathtied grewn'd the proction,\nWhile hear theirf, as do riscts is\nHake qwith alews there reviself and me\nThis she'd fearstainss, atis on of a wentrets of hather.\n\nTESCIWALLA:\nBut muck, sweet ungbend my thousandle itsimblie mos.\n\nThat Come; midesqoratit theirselveonts coondans.\n\nCARTILANUS:\nAnd no should did scrausited lip, the so:\nAnd that many disstons must far tur cangion;\nEdwack ablies, and shall a might\nThat jlaure of goods and mothers long stremites\nWith trowons my\n","output_type":"stream"}]}]}