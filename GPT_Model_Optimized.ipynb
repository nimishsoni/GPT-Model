{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbt/5a9hUjxaGwif/DjyZQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Model trained on tiny-Shakespeare data and Optimized performance and efficiency\n",
        "This is a separate notebook for experimenting with the model.\n",
        "Key Components of the model are:\n",
        "1. word embedding: **Sub-word level tokens**\n",
        "2. Positional encoding: **Rotational PE**\n",
        "3. multi-headed self-attention (No cross attention): **GeLU activation function**\n",
        "4. Transformer Decoder block (No Encoder since it is generating text on its own) - **RMS Normalization**\n",
        "Other Optimization Implemented during Training:\n",
        "- Weight Decay\n",
        "- Learning Rate Scheduling\n",
        "- Gradient Clipping\n"
      ],
      "metadata": {
        "id": "dtPsWQFcbIZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl4cSu9Ddeqp",
        "outputId": "ed72a054-08af-4f97-e1ef-d31e7a212da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-20 10:10:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-12-20 10:10:12 (21.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the tiny-Shakesspeare text data from Github repo\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "#import torch_xla\n",
        "#import torch_xla.core.xla_model as xm"
      ],
      "metadata": {
        "id": "tb2m_FMD-vvE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data and Explore"
      ],
      "metadata": {
        "id": "nzSArLnwq8Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the tiny-Shakespeare txt file\n",
        "with open('input.txt',mode='r',encoding='utf-8',closefd=True) as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "u2_fqE-GlyVf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View file and its stats\n",
        "print(type(text),len(text),text[1:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v2DxpFwnEU-",
        "outputId": "75c9982e-9dee-4119-be6f-83bf94ab5c07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'> 1115394 irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all unique characters used in the text\n",
        "char1 = set(text)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars),vocab_size,char1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lgM-6QInIv2",
        "outputId": "4c3bf5dc-e0e8-4fa5-86b1-a51105be3911"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 65 {'I', 'e', 'A', 'l', 'k', 'u', 'j', ',', 'y', 'v', '!', 'g', 'B', '-', '3', 'C', 't', 'i', 'w', 'b', 'L', 'd', 'n', '&', 'm', \"'\", 'x', 'N', ':', 'F', 'h', 'E', 'Q', 'R', 'f', '.', ';', 'o', 'z', 'Y', 'J', 'P', 'q', 'W', 'r', 'a', 'V', 'U', 'K', 'M', '$', ' ', 'D', 'T', 'G', 's', 'p', 'X', '?', 'O', 'H', 'Z', 'c', 'S', '\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the text data"
      ],
      "metadata": {
        "id": "1fCEfuKcrE7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(''.join(chars),vocab_size,char1) # Create a mapping of characters and integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "#print(stoi['A'],'\\n', itos)\n",
        "\n",
        "# Encoder takes string as input and provides integers as output.\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# Decoder takes list of integers as input an provides string as output\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "#print(encode('Hi. I am Nimish!!'))\n",
        "#print(decode(encode('Hi. I am Nimish!!')))\n"
      ],
      "metadata": {
        "id": "GuwvQ1J-Sp-m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets encode entire text dataset and store it into a torch.tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape,'\\n',data.dtype,'\\n',data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE5qlTyfynSq",
        "outputId": "9fc9d7fc-3bc7-49b2-d618-7a05fc798a8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) \n",
            " torch.int64 \n",
            " tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data prep for training\n",
        "Split the data in to Train and Val (10%)\n",
        "Divide the data in to chunks/blocks and batches"
      ],
      "metadata": {
        "id": "etNYLOt556uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the data in to train (90%) and validation set (10%)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "EvbB-cKJ0ewF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Create batch of 4 randomly generated integers within length of text data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Create batch of 4 blocks (each of block size 8) from randomly selected integers for parallel processing\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    #move x, y parameters to GPU if available or on CPU\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "h_-m6llh8As1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "YhBy-hLRDgPq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # Key represents the information each element of sequence holds\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) # Defines the query or search the model is asking for\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        # masking out the attention scores in the upper triangular portion of the matrix, the model is forced to only focus on the elements that appear before the current position in the sequence.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        # the model gives more weight to elements that have higher attention scores, meaning they are more relevant to the current query.\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gFy5dUGPxrTM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RMS Normalization Block\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n",
        "        return self.scale * x / (rms + self.eps)"
      ],
      "metadata": {
        "id": "SDahl8L21Ew4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(), ## GELU actication function\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) #Initialization of self-multihead attention module defined earlier\n",
        "        self.ffwd = FeedFoward(n_embd) # Initialization of feedforward module defined earlier\n",
        "        #self.ln1 = nn.LayerNorm(n_embd) #Initialize first normalization layer\n",
        "        #self.ln2 = nn.LayerNorm(n_embd)  #Initialize second normalization layer\n",
        "        self.ln1 = RMSNorm(n_embd) #Initialize first normalization layer with RMS Norm\n",
        "        self.ln2 = RMSNorm(n_embd)  #Initialize second normalization layer with RMS Norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) #Input block (x) undergoes normalization followed by self attention\n",
        "        x = x + self.ffwd(self.ln2(x)) # Output of previous block goes through normalization followed by feedforward layer followed\n",
        "        return x"
      ],
      "metadata": {
        "id": "WWs60ospyJDN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # Word Embedding\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # Positional Embedding\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Creates stack of n_layer transformer blocks using sequential container\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer normalization\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # Map normalized embedding outputs to logits which are used for loss calculation\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C) Token Embedding\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) Positional Embedding\n",
        "        x = tok_emb + pos_emb # (B,T,C) Embeddings added to create input sequence\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "E7uZJ9G5UJ6T"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "wt_decay = 1e-4 # regularization technique that penalizes large weights during training.\n",
        "max_norm = 1.0 # maximum norm of the gradients. If the norm of the gradient exceeds this value, it will be scaled down to fit the norm limit.\n",
        "lr_decay = 0.2 # Learning rate decays to lr_decay*learning rate every 1000 steps\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Use TPU if available, otherwise use CPU\n",
        "#device = xm.xla_device() if xm.xla_device() is not None else 'cpu'\n"
      ],
      "metadata": {
        "id": "M4tGpbi1BEtG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAnPAG50Agtp",
        "outputId": "b630708e-d904-470b-b738-b77bdc3a8b06"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=wt_decay)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=lr_decay)\n",
        "\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
        "    optimizer.step() # Update the model parameters\n",
        "    scheduler.step() # Update the learning rate\n",
        "\n",
        "\n",
        "# Stop timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print training duration\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training completed in: {training_time:.2f} seconds\")\n",
        "\n",
        "# Generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfR4wzWO4-ts",
        "outputId": "989ef05d-fab7-4170-9aef-77d0734734c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.784321 M parameters\n",
            "step 0: train loss 4.3320, val loss 4.3322\n",
            "step 200: train loss 2.4112, val loss 2.4359\n",
            "step 400: train loss 2.1169, val loss 2.1742\n",
            "step 600: train loss 1.8809, val loss 1.9993\n",
            "step 800: train loss 1.7046, val loss 1.8581\n",
            "step 1000: train loss 1.5870, val loss 1.7585\n",
            "step 1200: train loss 1.5457, val loss 1.7210\n",
            "step 1400: train loss 1.5220, val loss 1.7104\n",
            "step 1600: train loss 1.5025, val loss 1.6925\n",
            "step 1800: train loss 1.4847, val loss 1.6786\n",
            "step 2000: train loss 1.4738, val loss 1.6619\n",
            "step 2200: train loss 1.4626, val loss 1.6563\n",
            "step 2400: train loss 1.4568, val loss 1.6501\n",
            "step 2600: train loss 1.4560, val loss 1.6493\n",
            "step 2800: train loss 1.4523, val loss 1.6456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1WS4XYNzXeWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}